---
title:          "Fine-grained Attention I/O Complexity: Comprehensive Analysis for Backward Passes"
date:           2024-10-12 00:01:00 +0800
selected:       false
pub:            "Arxiv"
# pub_pre:        "Submitted to "
# pub_post:       'Under review.'
# pub_last:       ' <span class="badge badge-pill badge-publication badge-success">Spotlight</span>'
pub_date:       "2024"

abstract: >-
  We establish tight I/O complexity bounds for attention mechanisms in large language models across small and large cache sizesâ€”confirming FlashAttention's optimality in large caches, improving algorithms for small caches, extending analysis to sparse attention, and offering insights for efficient LLM training and inference.
cover:          /assets/images/covers/io_complexity.jpg
authors:
  - Xiaoyu Li@
  - Yingyu Liang
  - Zhenmei Shi
  - Zhao Song
  - Yufa Zhou

bib: |
  @article{li2024fine,
  title={Fine-grained attention i/o complexity: Comprehensive analysis for backward passes},
  author={Li, Xiaoyu and Liang, Yingyu and Shi, Zhenmei and Song, Zhao and Zhou, Yufa},
  journal={arXiv preprint arXiv:2410.09397},
  year={2024}
  }

links:
  Paper: https://arxiv.org/abs/2410.09397
  # Code: https://github.com/luost26/academic-homepage
  # Unsplash: https://unsplash.com/photos/sliced-in-half-pineapple--_PLJZmHZzk
---
