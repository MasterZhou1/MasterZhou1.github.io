---
title:          "Differentially private attention computation"
date:           2024-10-14 00:01:00 +0800
selected:       false
pub:            "NeurIPS 2024 Workshop: Safe Generative AI"
# pub_pre:        "Submitted to "
# pub_post:       'Under review.'
# pub_last:       ' <span class="badge badge-pill badge-publication badge-success">Spotlight</span>'
# pub_date:       "2024"

abstract: >-
  We propose an efficient algorithm to approximate the attention matrix in Transformer-based large language models with differential privacy guarantees, addressing security and privacy concerns by preventing leakage of sensitive information during inferenceâ€”building on advancements in fast attention computation and differentially private matrix publishing.
# cover:          /assets/images/covers/tat.jpg
authors:
  - Yeqi Gao@
  - Zhao Song
  - Xin Yang
  - Yufa Zhou
  
bib: |
  @inproceedings{gao2024differentially,
    title={Differentially Private Attention Computation},
    author={Yeqi Gao and Zhao Song and Xin Yang and Yufa Zhou},
    booktitle={Neurips Safe Generative AI Workshop 2024},
    year={2024},
    url={https://openreview.net/forum?id=dj70ulvXDo}
  }

links:
  Paper: https://arxiv.org/abs/2305.04701
  # Code: https://github.com/luost26/academic-homepage
  # Unsplash: https://unsplash.com/photos/sliced-in-half-pineapple--_PLJZmHZzk
---
