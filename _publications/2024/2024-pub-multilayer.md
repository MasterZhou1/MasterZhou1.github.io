---
title:          "Multi-Layer Transformers Gradient Can be Approximated in Almost Linear Time
"
date:           2024-08-23 00:01:00 +0800
selected:       true
pub:            "NeurIPS 2024 Workshop: Optimization for Machine Learning"
# pub_pre:        "Submitted to "
# pub_post:       'Under review.'
# pub_last:       ' <span class="badge badge-pill badge-publication badge-success">Spotlight</span>'
# pub_date:       "2024"

abstract: >-
  We prove that gradients in multi-layer transformer models can be computed in almost linear time $n^{1+o(1)}$ using a novel fast approximation method with polynomially small error, overcoming the quadratic complexity bottleneck of self-attention and enabling more efficient training and deployment of long-context language models with general loss functions and common sub-modules like residual connections, causal masks, and multi-head attention.
# cover:          /assets/images/covers/tat.jpg
authors:
  - Yingyu Liang@
  - Zhizhou Sha
  - Zhenmei Shi
  - Zhao Song
  - Yufa Zhou
links:
  Paper: https://arxiv.org/abs/2408.13233
  Poster: /assets/pdfs/MultiLayer_OPT_Poster.pdf
  # Code: https://github.com/luost26/academic-homepage
  # Unsplash: https://unsplash.com/photos/sliced-in-half-pineapple--_PLJZmHZzk
---
