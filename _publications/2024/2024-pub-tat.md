---
title:          "Tensor attention training: Provably efficient learning of higher-order transformers"
date:           2024-05-26 00:01:00 +0800
selected:       true
pub:            "NeurIPS 2024 Workshop: Adaptive Foundation Models: Evolving AI for Personalized and Efficient Learning"
# pub_pre:        "Submitted to "
# pub_post:       'Under review.'
# pub_last:       ' <span class="badge badge-pill badge-publication badge-success">Spotlight</span>'
# pub_date:       "2024"

abstract: >-
  We prove that, under bounded entries, the backward gradient of tensor attention can be computed in almost linear time—overcoming the $O(n^3)$ complexity barrier—and propose efficient methods to enable practical higher-order transformer training with tensor attention architectures.
cover:          /assets/images/covers/high_order_attention.pdf
authors:
  - Yingyu Liang@
  - Zhenmei Shi
  - Zhao Song
  - Yufa Zhou
links:
  Paper: https://arxiv.org/abs/2405.16411
  # Code: https://github.com/luost26/academic-homepage
  # Unsplash: https://unsplash.com/photos/sliced-in-half-pineapple--_PLJZmHZzk
---
