---
title:          "Beyond Linear Approximations: A Novel Pruning Approach for Attention Matrix"
date:           2025-1-22 00:01:00 +0800
selected:       true
pub:            "ICLR"
# pub_pre:        "Submitted to "
# pub_post:       'Under review.'
# pub_last:       ' <span class="badge badge-pill badge-publication badge-success">Spotlight</span>'
pub_date:       "2025"

abstract: >-
  We introduce a novel LLM weight pruning method that directly optimizes for approximating the non-linear attention matrix—with theoretical convergence guarantees—effectively reducing computational costs while maintaining model performance.
cover:          /assets/images/covers/attn_prune.jpg
authors:
  - Yingyu Liang@
  - Jiangxuan Long
  - Zhenmei Shi
  - Zhao Song
  - Yufa Zhou
links:
  Paper: https://arxiv.org/abs/2410.11261
  # Code: https://github.com/luost26/academic-homepage
  # Unsplash: https://unsplash.com/photos/sliced-in-half-pineapple--_PLJZmHZzk
---
