---
title:          "Beyond Linear Approximations: A Novel Pruning Approach for Attention Matrix"
date:           2024-10-15 00:01:00 +0800
selected:       false
pub:            "Arxiv"
# pub_pre:        "Submitted to "
# pub_post:       'Under review.'
# pub_last:       ' <span class="badge badge-pill badge-publication badge-success">Spotlight</span>'
pub_date:       "2024"

abstract: >-
  We introduce a novel LLM weight pruning method that directly optimizes the approximation of the non-linear attention matrix—with theoretical convergence guarantees—effectively reducing computational costs while maintaining model performance, thus enabling efficient deployment on resource-constrained devices.
cover:          /assets/images/covers/attn_prune.jpg
authors:
  - Yingyu Liang@
  - Jiangxuan Long
  - Zhenmei Shi
  - Zhao Song
  - Yufa Zhou
links:
  Paper: https://arxiv.org/abs/2410.11261
  # Code: https://github.com/luost26/academic-homepage
  # Unsplash: https://unsplash.com/photos/sliced-in-half-pineapple--_PLJZmHZzk
---
