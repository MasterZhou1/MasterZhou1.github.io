---
title:          "Why Do Transformers Fail to Forecast Time Series In-Context?"
date:           2025-10-13 00:01:00 +0800
selected:       true
pub:            "NeurIPS 2025 Workshop: What Can('t) Transformers Do? "
pub_last:       '<span class="badge badge-pill badge-publication badge-success">Oral (3/68 â‰ˆ 4.4%)</span>'
pub_date:       "2025"

abstract: >-
  We investigate why Transformers underperform in time-series forecasting, proving under in-context learning theory that linear self-attention cannot beat classical AR((p)) predictors and that chain-of-thought inference collapses to mean predictions, with experiments confirming these theoretical limits.

cover:          /assets/images/covers/icl-tsf.png
authors:
  - Yufa Zhou*
  - Yixiao Wang*
  - Surbhi Goel
  - Anru R. Zhang

links:
  # Paper: TODO
  Code: https://github.com/MasterZhou1/ICL-Time-Series
---
